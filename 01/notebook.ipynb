{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Overview\n",
    "\n",
    "Twitter is a mix of social network and microblogging. In this network, people post information and communicate among themselves through messages, called tweets, that can contain up to 280 characters. In this assignment, *we will implement a prototype that can detect if an airline company is positively or negatively mentioned in a tweet*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# 2 - Sentiment Analysis Model (13 points)\n",
    "\n",
    "In the literature, the task of extracting the sentiment of a text is called *sentiment analysis*. We will implement a bag-of-words (BoW) model for this task.\n",
    "\n",
    "## 2.1 -  Setup\n",
    "\n",
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quentin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/quentin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/quentin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/quentin/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n",
    "\n",
    "# python\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"universal_tagset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Dataset\n",
    "\n",
    "Please download the zip file in the following url: https://drive.google.com/file/d/1iGmESwPXpO3sIZFGOCrysxJ27AHdly-Y/view?usp=sharing\n",
    "\n",
    "In this zip file, there are 2 files:\n",
    "1. train.tsv: training dataset\n",
    "2. dev.tsv: validation dataset\n",
    "\n",
    "Each line of the files has the following information about a tweet: *tweet id*, *user id*, *label* and *message text*.\n",
    "\n",
    "There are three labels in the dataset: *negative*, *neutral* and *positive*. We represent each one of these labels as 0, 1 and 2 respectively.\n",
    "\n",
    "In the code above read the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    dtFile = codecs.open(path, \"r\")\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for l in dtFile:\n",
    "        sid, uid, label, text = re.split(r\"\\s+\", l, maxsplit=3)\n",
    "\n",
    "        text = text.strip()\n",
    "\n",
    "        # Remove not available\n",
    "        if text == \"Not Available\":\n",
    "            continue\n",
    "\n",
    "        x.append(text)\n",
    "\n",
    "        if label == \"negative\":\n",
    "            y.append(0)\n",
    "        elif label == \"neutral\":\n",
    "            y.append(1)\n",
    "        elif label == \"positive\":\n",
    "            y.append(2)\n",
    "\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Path of training dataset\n",
    "trainingPath = \"data/train_data.tsv\"\n",
    "\n",
    "# Path of validation dataset\n",
    "validationPath = \"data/dev_data.tsv\"\n",
    "\n",
    "training_X, training_Y = load_dataset(trainingPath)\n",
    "validation_X, validation_Y = load_dataset(validationPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 2.3.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 2.3.1.1 - Question 1 (0.5 point) \n",
    "\n",
    "Implement the SpaceTokenizer and NLTKTokenizer tokenizers: \n",
    "- **SpaceTokenizer** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **NLTKTokenizer** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "\n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Write your code here\n",
    "\n",
    "        # Have to return a list of tokens\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Write your code here\n",
    "\n",
    "        tknzr = TweetTokenizer()\n",
    "\n",
    "        # Have to return a list of tokens\n",
    "        return tknzr.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'of', 'NLP!!!']\n",
      "['hello', 'world', 'of', 'NLP', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "ST = SpaceTokenizer()\n",
    "print(ST.tokenize(\"hello\\tworld of\\nNLP!!!\"))\n",
    "\n",
    "NLTKT = NLTKTokenizer()\n",
    "print(NLTKT.tokenize(\"hello\\tworld of\\nNLP!!!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 - Stemming\n",
    "\n",
    "In the tweets *\"I should have bought a new shoes today\"* and *\"I spent too much money buying games\"*, the words *\"buy\"* and *\"bought\"* represent basically the same concept. Considering both words as different can unnecessarily increase the dimensionality of the problem and can negatively impact the performance of simple models. Therefore, a unique form (e.g., the root buy) can represent both words. The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*.\n",
    "\n",
    "#### 2.3.2.1 - Question 2 (0.5 point) \n",
    "\n",
    "Retrieve the stems of the tokens using the attribute *stemmer* from the class *Stemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        # Have to return a list of stems\n",
    "        return self.stemmer.stem(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'buy', 'a', 'car', ',', 'he', 'buy', 'at', 'http://google.com', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I buy a car, he buys at http://google.com !!!\"\n",
    "tokens = NLTKTokenizer().tokenize(tweet)\n",
    "tokens = list(map(Stemmer().stem, tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 - Twitter preprocessing\n",
    "\n",
    "Sometimes only applying the default NLP preprocessing steps is not enough. Data for certain domains can have peculiar characteristics which requires specific preprocessing steps to remove the noise and create a more suitable format for the models. \n",
    "\n",
    "In NLP, methods store a set of words, called dictionary, and all the words out of the dictionary are considered as unknown. In this assignment, the feature space dimensionality of a model is directly related to the number of words in the dictionary. Since high-dimensional spaces can suffer from the curse of dimensionality, our goal is to create preprocessing steps that decrease vocabulary size.  \n",
    "\n",
    "#### 2.3.3.1 - Question 3 (2.0 points)\n",
    "\n",
    "Briefly explain and implement at least two preprocessing steps that reduce the dictionary size (number of unique words). These preprocessing steps must be related to the specific characteristic of the Twitter data. Therefore, for instance, the stop words removal will not be accepted as a preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove links and url\n",
    "* Remove numbers \n",
    "* Remove # (keep words)\n",
    "* Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "class TwitterPreprocessing(object):\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        new_tweet = []\n",
    "        # Write your preprocessing steps here.\n",
    "        for word in tweet:\n",
    "            # remove punctuation\n",
    "            if any(c in string.punctuation for c in word):\n",
    "                continue\n",
    "            # remove url and username\n",
    "            if '@' in word or 'http' in word:\n",
    "                continue\n",
    "            # remove word if digit inside\n",
    "            if any(letter.isdigit() for letter in word):\n",
    "                continue\n",
    "            # check if empty\n",
    "            if not word.strip():\n",
    "                continue\n",
    "            new_tweet.append(word.replace('#', ''))\n",
    "\n",
    "        # return the preprocessed twitter\n",
    "        return new_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'buy', 'a', 'car', 'he', 'buy', 'at']\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I buy a car, he buys at http://google.com !!!\"\n",
    "tokens = NLTKTokenizer().tokenize(tweet)\n",
    "tokens = list(map(Stemmer().stem, tokens))\n",
    "tokens = TwitterPreprocessing().preprocess(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3  Pipeline\n",
    "\n",
    "The pipeline is sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. We implement the class *PreprocessingPipeline* that apply the tokenizer, twitter preprocessing and stemer to the text.\n",
    "\n",
    "**Feel free to change the preprocessing order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing(\n",
    "        ) if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        if self.stemmer:\n",
    "            tokens = list(map(self.stemmer.stem, tokens))\n",
    "\n",
    "        if self.twitterPreprocesser:\n",
    "            tokens = self.twitterPreprocesser.preprocess(tokens)\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gas', 'by', 'my', 'hous', 'hit', 'go', 'to', 'chapel', 'hill', 'on', 'sat']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "print(list(map(pipeline.preprocess, training_X[:1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 N-grams\n",
    "\n",
    "An n-gram is a contiguous sequence of *n* tokens from a text. Thus, for instance,the sequence *\"bye as\"* and *\"walked through\"* are example of 2-grams from the sentence *\"He said bye as he walked through the door .\"*. 1-gram, 2-gram and 3-gram are, respectively, called unigram, bigram and trigram. We list all the possible unigram, bigram and trigram from the *\"He said bye as he walked through the door .\"*:\n",
    "\n",
    "- Unigram: [\"He\", \"said\", \"bye\", \"as\", \"he\", \"walked\", \"through\", \"the\", \"door\", \".\"]\n",
    "- Bigram: [\"He said\", \"said bye\", \"bye as\", \"as he\", \"he walked\", \"walked through\", \"through the\", \"the door\", \"door .\"] \n",
    "- Trigram: [\"He said bye\", \"said bye as\", \"bye as he\", \"as he walked\", \"he walked through\", \"walked through the\", \"through the door\", \"the door .\"] \n",
    "\n",
    "\n",
    "### 2.4.1 - Question 4 (1 point)\n",
    "\n",
    "Implement bigram and trigram.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    # This function returns the list of bigrams\n",
    "    return [\" \".join(tokens[i:i + 2]) for i in range(len(tokens) - 1)]\n",
    "\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    # This function returns the list of trigrams\n",
    "    return [\" \".join(tokens[i:i + 3]) for i in range(len(tokens) - 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bag-of-words\n",
    "\n",
    "Logistic regression, SVM and other well-known models only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Pandemic is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "|            | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 2.5.1 - Question 5 (2 points)\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, relates each unigram, bigram or trigram to a specific integer and  \n",
    "        transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "        \n",
    "        X: a list that contains tweet contents\n",
    "        \n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        X = list(map(self.pipeline.preprocess, X))\n",
    "        if self.bigram:\n",
    "            for tweet in X:\n",
    "                tweet += bigram(tweet)\n",
    "        if self.trigram:\n",
    "            for tweet in X:\n",
    "                tweet += trigram(tweet)\n",
    "        N = len(X)\n",
    "\n",
    "        # list of all words\n",
    "        self.words = list(set((word for tweet in X for word in tweet)))\n",
    "\n",
    "        data, row, col = [], [], []\n",
    "        for i, tweet in enumerate(X):\n",
    "#             print(\"\\r{}/{}\".format(i, N), end=\"\")\n",
    "            for word in tweet:\n",
    "                data.append(tweet.count(word))\n",
    "                row.append(i)\n",
    "                col.append(self.words.index(word))\n",
    "\n",
    "        return csr_matrix((data, (row, col)), shape=(N, len(self.words)))\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and  transforms the text in a list of integer.\n",
    "        Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "        X: a list of vectors\n",
    "\n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        X = list(map(self.pipeline.preprocess, X))\n",
    "        if self.bigram:\n",
    "            for tweet in X:\n",
    "                tweet += bigram(tweet)\n",
    "        if self.trigram:\n",
    "            for tweet in X:\n",
    "                tweet += trigram(tweet)\n",
    "        N = len(X)\n",
    "\n",
    "        data, row, col = [], [], []\n",
    "        for i, tweet in enumerate(X):\n",
    "            for word in tweet:\n",
    "                if word in self.words:\n",
    "                    data.append(tweet.count(word))\n",
    "                    row.append(i)\n",
    "                    col.append(self.words.index(word))\n",
    "\n",
    "        return csr_matrix((data, (row, col)), shape=(N, len(self.words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of tweets. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.2.1 - Question 6 (3 points)\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "        transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "        \n",
    "        X: a list that contains tweet contents\n",
    "        \n",
    "        :return: a list that contains the list of integers\n",
    "        \"\"\"\n",
    "        X = list(map(self.pipeline.preprocess, X))\n",
    "        if self.bigram:\n",
    "            for tweet in X:\n",
    "                tweet += bigram(tweet)\n",
    "        if self.trigram:\n",
    "            for tweet in X:\n",
    "                tweet += trigram(tweet)\n",
    "        N = len(X)\n",
    "\n",
    "        # list of all words\n",
    "        self.words = list(set((word for tweet in X for word in tweet)))\n",
    "\n",
    "        # compute idf\n",
    "        self.idf = []\n",
    "        for word in self.words:\n",
    "            count = sum([word in tweet for tweet in X])\n",
    "            self.idf.append(math.log(N / count))\n",
    "\n",
    "        data, row, col = [], [], []\n",
    "        for i, tweet in enumerate(X):\n",
    "            for word in set(tweet):\n",
    "                data.append(tweet.count(word) * self.idf[i])\n",
    "                row.append(i)\n",
    "                col.append(self.words.index(word))\n",
    "\n",
    "        return csr_matrix((data, (row, col)), shape=(N, len(self.words)))\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and  \n",
    "            transforms the text in a list of integer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        X = list(map(self.pipeline.preprocess, X))\n",
    "        if self.bigram:\n",
    "            for tweet in X:\n",
    "                tweet += bigram(tweet)\n",
    "        if self.trigram:\n",
    "            for tweet in X:\n",
    "                tweet += trigram(tweet)\n",
    "        N = len(X)\n",
    "\n",
    "        data, row, col = [], [], []\n",
    "        for i, tweet in enumerate(X):\n",
    "            for word in set(tweet):\n",
    "                if word in self.words:\n",
    "                    data.append(tweet.count(word) * self.idf[i])\n",
    "                    row.append(i)\n",
    "                    col.append(self.words.index(word))\n",
    "\n",
    "        return csr_matrix((data, (row, col)), shape=(N, len(self.words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Classifier using BoW\n",
    "\n",
    "We are going to use logistic regression as a classifier. Read the following page to now more about this classifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "\n",
    "The method *train_evaluate* trains and evaluates the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.6.1 - Question 7 (4 points)\n",
    "\n",
    "Train and calculate the logistic regression accuracy in the *training and validation dataset* using each one of the following configurations:\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "Besides the accuracy, you have to report the dictionary size for each one of configurations. Finally, describe the results found by you and answer the following questions:\n",
    "- Which preprocessing has helped the model? Why?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? \n",
    "- Has the bigram and trigram improved the performance? If yes, can you mention the reasons of this improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience:  trainAcc   validAcc \n",
      "         1:  99.545%    59.312%  \n",
      "         2:  99.076%    62.489%  \n",
      "         3:  99.927%    63.107%  \n",
      "         4:  99.795%    58.694%  \n",
      "         5:  99.853%    60.989%  \n",
      "         6:  99.853%    61.430%  \n"
     ]
    }
   ],
   "source": [
    "print(\"{:10}: {:^10} {:^10}\".format(\"Experience\", \"trainAcc\", \"validAcc\"))\n",
    "# 1. CountBoW + SpaceTokenizer(without tokenizer) + unigram\n",
    "bowObj = CountBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=False, twitterPreprocessing=False, stemming=False))\n",
    "_, trainAcc, validAcc = train_evaluate(training_X, training_Y, validation_X,\n",
    "                                       validation_Y, bowObj)\n",
    "print(\"{:10}: {:^10.3%} {:^10.3%}\".format(1, trainAcc, validAcc))\n",
    "\n",
    "# 2. CountBoW + NLTKTokenizer + unigram\n",
    "bowObj = CountBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=True, twitterPreprocessing=False, stemming=False))\n",
    "_, trainAcc, validAcc = train_evaluate(training_X, training_Y, validation_X,\n",
    "                                       validation_Y, bowObj)\n",
    "print(\"{:10}: {:^10.3%} {:^10.3%}\".format(2, trainAcc, validAcc))\n",
    "\n",
    "# 3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "bowObj = TFIDFBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=True, twitterPreprocessing=False, stemming=True))\n",
    "_, trainAcc, validAcc = train_evaluate(training_X, training_Y, validation_X,\n",
    "                                       validation_Y, bowObj)\n",
    "print(\"{:10}: {:^10.3%} {:^10.3%}\".format(3, trainAcc, validAcc))\n",
    "\n",
    "# 4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "bowObj = TFIDFBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=True, twitterPreprocessing=True, stemming=True))\n",
    "_, trainAcc, validAcc = train_evaluate(training_X, training_Y, validation_X,\n",
    "                                       validation_Y, bowObj)\n",
    "print(\"{:10}: {:^10.3%} {:^10.3%}\".format(4, trainAcc, validAcc))\n",
    "\n",
    "# 5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "bowObj = TFIDFBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=True, twitterPreprocessing=True, stemming=True),\n",
    "    bigram=True)\n",
    "_, trainAcc, validAcc = train_evaluate(training_X, training_Y, validation_X,\n",
    "                                       validation_Y, bowObj)\n",
    "print(\"{:10}: {:^10.3%} {:^10.3%}\".format(5, trainAcc, validAcc))\n",
    "\n",
    "# 6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "bowObj = TFIDFBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=True, twitterPreprocessing=True, stemming=True),\n",
    "    bigram=True,\n",
    "    trigram=True)\n",
    "_, trainAcc, validAcc = train_evaluate(training_X, training_Y, validation_X,\n",
    "                                       validation_Y, bowObj)\n",
    "print(\"{:10}: {:^10.3%} {:^10.3%}\".format(6, trainAcc, validAcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model chosen : the 3rd one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "bowObj = TFIDFBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=True, twitterPreprocessing=False, stemming=True))\n",
    "classifier, _, _ = train_evaluate(training_X, training_Y, validation_X,\n",
    "                                  validation_Y, bowObj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prototype (7 points)\n",
    "\n",
    "During the last years, *E Corp* has collected tweets to create a dataset to their sentiment analysis tool. Now, airline companies have contracted *E Corp* to analyze the consumer opinion about them. Your job is to extract information from the tweet database about the following companies: Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.\n",
    "\n",
    "*For the prototype, you have to use the best model found in the Section 2.*\n",
    "\n",
    "## 3.1 Dataset\n",
    "\n",
    "In https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing, you can find the raw tweet retrieved by E corp.  Each tweet is represented as json that the have attributes listed in the page https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.\n",
    "\n",
    "** You will answer the question of this section using this tweet database (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing).**\n",
    "\n",
    "## 3.2 Sentiment Analysis\n",
    "\n",
    "\n",
    "### 3.2.1 Question 8 (0.5 point)\n",
    "\n",
    "Implement the method *extract_tweet_content* that extracts the content of each tweet in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def extract_tweet_content(raw_tweet_file):\n",
    "    \"\"\"\n",
    "    Extract the tweet content for each json object\n",
    "    \n",
    "    raw_tweet_file: file path that contains all json objects\n",
    "    \n",
    "    :return: a list with the tweet contents\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for tweet in open(raw_tweet_file, \"r\"):\n",
    "        json_tweets = json.loads(tweet)\n",
    "        results.append(json_tweets[\"text\"])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = extract_tweet_content(\"data/e_corp_dataset.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's the last day of IT class! Whew~ I really did not like this professer @ all!!! \n"
     ]
    }
   ],
   "source": [
    "print(tweets_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Question 9 (1 points)\n",
    "\n",
    "Implement the method *detect_airline* that detects the airline companies in a tweet. Besides that, explain your approach to detect the companies and its possible drawbacks.\n",
    "\n",
    "The detect_airline has to be able to return if none or more than one airline companies are mentioned in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_airline(tweet):\n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "    \n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    airline_names = [\n",
    "        [\"@americanair\", \"#americanair\", \"american air\", \"american_air\"],\n",
    "        [\"@delta\", \"#delta\", \"delta air\", \"delta_air\"],\n",
    "        [\"@southwestair\", \"#southwestair\", \"southwest air\", \"southwest_air\"],\n",
    "        [\"@united\", \"#united\", \"united air\", \"united_air\"],\n",
    "        [\"@virginamerica\", \"#virginamerica\", \"virgin america\", \"virgin_america\"],\n",
    "        [\"@airfrance\", \"#airfrance\", \"air france\", \"air_france\"],\n",
    "        [\"@british_airways\", \"#british_airways\", \"british air\"],\n",
    "        [\"@usairways\", \"#usairways\", \"us air\", \"us_air\"]\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        names[0]\n",
    "        for names in airline_names\n",
    "        if any([n in tweet.lower() for n in names])\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.1 Question 10 (0.5 points)\n",
    "\n",
    "Implement the method *extract_sentiment* that receives a tweet and extracts its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(classifier, tweets):\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    return classifier.predict(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Question 11 (2 points)\n",
    "\n",
    "Using the *extract_tweet_content*, *detect_airline* and *extract_sentiment*, implement a code that generates a bar chart that contains the number of positive, neutral and negatives tweets for each one of the companies. Briefly describe your bar chart (e.g, which was the company with most negative tweets) and how this chart can help airline companies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_names = [\n",
    "    \"@americanair\", \"@delta\", \"@southwestair\", \"@united\", \"@virginamerica\",\n",
    "    \"@airfrance\", \"@british_airways\", \"@usairways\"\n",
    "]\n",
    "\n",
    "data = [[] for _ in range(len(airline_names))]\n",
    "\n",
    "for i, tweet in enumerate(tweets_list, 1):\n",
    "    airlines = detect_airline(tweet)\n",
    "    if airlines:\n",
    "        for a in airlines:\n",
    "            data[airline_names.index(a)].append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@americanair : 2978\n",
      "@delta : 193\n",
      "@southwestair : 2491\n",
      "@united : 3901\n",
      "@virginamerica : 545\n",
      "@airfrance : 836\n",
      "@british_airways : 13\n",
      "@usairways : 3033\n",
      "\n",
      "Total tweets: 13990\n"
     ]
    }
   ],
   "source": [
    "for n, d in zip(airline_names, data):\n",
    "    print(\"{} : {}\".format(n, len(d)))\n",
    "\n",
    "print(\"\\nTotal tweets: {}\".format(sum([len(d) for d in data])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SouthwestAir Sure was,  with @Delta  looks like I'll make my birthday celebration after all.\n",
      "@SouthwestAir  goes to court to gain access to @Delta gates at Love Field  http://t.co/ILqzmMJiYQ #deltanews\n",
      "@SouthwestAir I'm scheduled to come back tomorrow, finally but I don't know SWA hasn't flown out of @Fly_Nashville yet but @Delta has\n",
      "haha. @delta_goodrem quantum physics ? wth is that. yes. prime-minister Delta  i stayed up late reading gossipgirl :S youknowyouloveme &lt;3\n",
      "@Deltachild I'd share mine, if I had any left! \n"
     ]
    }
   ],
   "source": [
    "print(*data[1][:5], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_data = [bowObj.transform(airline_tweet) for airline_tweet in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [extract_sentiment(classifier, bow) for bow in bow_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    unique, counts = numpy.unique(r, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        if u == 0:\n",
    "            plt.bar(i - 0.25, c, 0.25, color=\"red\")\n",
    "        if u == 1:\n",
    "            plt.bar(i, c, 0.25, color=\"grey\")\n",
    "        if u == 2:\n",
    "            plt.bar(i + 0.25, c, 0.25, color=\"blue\")\n",
    "\n",
    "plt.xticks(list(range(len(airline_names))))\n",
    "plt.gca().set_xticklabels(airline_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Term Analysis\n",
    "\n",
    "POS-tagging consists of extracting the part-of-speech (POS) of each token in a sentence. For instance, the table below depicts the part-of-speechs of the sentence *The cat is white!* are.\n",
    "\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "The part-of-speech can be more complex than what we have learned in the school. Linguistics need to have a more detailed information about systax information of the words in a sentence. For our problem, we do not need this level of information and, thus, we will use a less complex set, called universal POS tags. \n",
    "\n",
    "In POS-tagging, each part-of-speech is represented by a tag. You can find the POS tag list used in this assignement at https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('cat', 'NOUN'), ('is', 'VERB'), ('white', 'ADJ'), ('!', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['The', 'cat', 'is',  'white', '!']\n",
    "nltk.pos_tag(s,tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Question 12 (2 points)\n",
    "\n",
    "**Implement a code** that retrieves the top 10 most frequent terms for each airline company. You will only consider the terms that appear in a positive and negative tweets. Besides that, we consider as term:\n",
    "1. Words that are either an adjective or a noun\n",
    "2. n-grams that are composed by adjectives followed by a noun (e.g., dirty place) or a noun followed by another noun (e.g.,sports club).\n",
    "\n",
    "Moreover, **generate a table** with the top 10 most frequent terms and their normalized frequencies(percentage) for each airline company.\n",
    "\n",
    "**Do not forget to remove the company names from the chart.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_neutral = []\n",
    "\n",
    "airline_names = [\n",
    "    \"@americanair\", \"#americanair\", \"american air\", \"american_air\", \"@delta\",\n",
    "    \"#delta\", \"delta air\", \"delta_air\", \"@southwestair\", \"#southwestair\",\n",
    "    \"southwest air\", \"southwest_air\", \"@united\", \"#united\", \"united air\",\n",
    "    \"united_air\", \"@virginamerica\", \"#virginamerica\", \"virgin america\",\n",
    "    \"virgin_america\", \"@airfrance\", \"#airfrance\", \"air france\", \"air_france\",\n",
    "    \"@british_airways\", \"#british_airways\", \"british air\", \"@usairways\",\n",
    "    \"#usairways\", \"us air\", \"us_air\"\n",
    "]\n",
    "\n",
    "pipeline = PreprocessingPipeline(\n",
    "    tokenization=True, twitterPreprocessing=False, stemming=True)\n",
    "\n",
    "for tweets, sentiments in zip(data, results):\n",
    "    tweets_without_neutral = []\n",
    "    process_tweets = map(pipeline.preprocess, tweets)\n",
    "    for tweet, sentiment in zip(process_tweets, sentiments):\n",
    "        if sentiment != 1:\n",
    "            new_tweet = [\n",
    "                word for word in tweet\n",
    "                if not any(word in name for name in airline_names)\n",
    "            ]\n",
    "            tweets_without_neutral.append(new_tweet)\n",
    "    data_without_neutral.append(tweets_without_neutral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@dfwairport', 'have', 'been', 'tri', 'to', 'get', 'canx', 'flight', 'resch', 'sinc', 'noon', '.', 'got', '2', 'call', 'back', 'but', 'call', 'disconnect', 'as', 'soon', 'as', 'answer']\n",
      "['after', 'all', ',', 'the', 'plane', \"didn't\", 'land', 'ident', 'or', 'wors', ')', 'condit', 'at', 'grk', 'accord', 'to', 'metar', '.']\n"
     ]
    }
   ],
   "source": [
    "print(*data_without_neutral[0][:2], sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_processing(tweet):\n",
    "    new_tweet = []\n",
    "\n",
    "    authorized = [\n",
    "        # unigram\n",
    "        \"NOUN\",\n",
    "        \"ADJ\",\n",
    "        #bigram\n",
    "        ('ADJ', 'NOUN'),\n",
    "        ('ADJ', 'NOUN'),\n",
    "        ('NOUN', 'NOUN'),\n",
    "        #trigram\n",
    "        ('ADJ', 'NOUN', 'NOUN'),\n",
    "        ('NOUN', 'NOUN', 'NOUN'),\n",
    "        ('ADJ', 'ADJ', 'NOUN'),\n",
    "        ('NOUN', 'ADJ', 'NOUN')\n",
    "    ]\n",
    "\n",
    "    for word, tag in nltk.pos_tag(tweet, tagset='universal'):\n",
    "        if tag in authorized:\n",
    "            new_tweet.append(word)\n",
    "\n",
    "    for i in range(len(tweet) - 1):\n",
    "        pos_bigram = list(\n",
    "            zip(*nltk.pos_tag(tweet[i:i + 2], tagset='universal')))\n",
    "        if pos_bigram[1] in authorized:\n",
    "            new_tweet.append(' '.join(pos_bigram[0]))\n",
    "\n",
    "    for i in range(len(tweet) - 2):\n",
    "        pos_bigram = list(\n",
    "            zip(*nltk.pos_tag(tweet[i:i + 3], tagset='universal')))\n",
    "        if pos_bigram[1] in authorized:\n",
    "            new_tweet.append(' '.join(pos_bigram[0]))\n",
    "    return new_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with only authorized data type\n",
    "data_only_auth = [list(map(pos_processing, d)) for d in data_without_neutral]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@dfwairport', 'canx', 'flight', 'resch', 'sinc', 'noon', 'call', 'disconnect', 'answer', 'canx flight', 'flight resch', 'resch sinc', 'sinc noon', 'call disconnect', 'canx flight resch', 'flight resch sinc', 'resch sinc noon']\n",
      "['plane', \"didn't\", 'land', 'ident', 'wors', 'condit', 'grk', 'accord', \"plane didn't\", \"didn't land\", 'land ident', 'grk accord', \"plane didn't land\"]\n"
     ]
    }
   ],
   "source": [
    "print(*data_only_auth[0][:2], sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'NOUN')\n",
      "(\"didn't\", 'NOUN')\n"
     ]
    }
   ],
   "source": [
    "# note that didn is considered a noun!\n",
    "print(*nltk.pos_tag([\"i\", \"didn't\"], tagset='universal'), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def n_frequent(tweets):\n",
    "    flat_tweets = [word for tweet in tweets for word in tweet]\n",
    "    vocab, counts = np.unique(flat_tweets, return_counts=True)\n",
    "    counts_vocab = sorted(zip(counts, vocab), reverse=True)\n",
    "    return counts_vocab[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------    American Air    ----------\n",
      "\tflight              :  792\n",
      "\tthank               :  295\n",
      "\tcancel              :  237\n",
      "\tservic              :  164\n",
      "\tcustom              :  164\n",
      "\tcancel flightl      :  162\n",
      "\tflightl             :  150\n",
      "\thour                :  134\n",
      "\t\"                   :  128\n",
      "\tcancel flight       :  126\n",
      "----------       Delta        ----------\n",
      "\t@delta_goodrem      :   36\n",
      "\tflight              :   28\n",
      "\ti'm                 :   15\n",
      "\ttime                :   13\n",
      "\tgood                :   12\n",
      "\tthank               :    9\n",
      "\tcustom              :    9\n",
      "\tsorri               :    8\n",
      "\tnew                 :    8\n",
      "\tcancel              :    8\n",
      "----------   Southwest Air    ----------\n",
      "\tflight              :  647\n",
      "\tthank               :  307\n",
      "\tcancel              :  176\n",
      "\ttime                :  108\n",
      "\tservic              :  108\n",
      "\tcustom              :  105\n",
      "\tcancel flightl      :  104\n",
      "\tcancel flight       :   97\n",
      "\thour                :   89\n",
      "\thold                :   88\n",
      "----------       United       ----------\n",
      "\tflight              :  910\n",
      "\tthank               :  387\n",
      "\tbag                 :  232\n",
      "\tservic              :  212\n",
      "\tcustom              :  203\n",
      "\tdelay               :  201\n",
      "\ttime                :  199\n",
      "\tcancel              :  145\n",
      "\thour                :  132\n",
      "\tairlin              :  129\n",
      "----------   Virgin America   ----------\n",
      "\tflight              :  100\n",
      "\tthank               :   45\n",
      "\tseat                :   21\n",
      "\tfli                 :   20\n",
      "\tbook                :   20\n",
      "\tgreat               :   19\n",
      "\tbest                :   17\n",
      "\tlove                :   16\n",
      "\tcancel              :   16\n",
      "\tairlin              :   16\n",
      "----------     Air France     ----------\n",
      "\tflight              :  172\n",
      "\tsad                 :  160\n",
      "\tplane               :  100\n",
      "\tfamili              :   85\n",
      "\tmiss                :   75\n",
      "\tpeopl               :   54\n",
      "\tpasseng             :   53\n",
      "\t..                  :   50\n",
      "\thope                :   48\n",
      "\tcrash               :   46\n",
      "----------  British Airways   ----------\n",
      "\tbag                 :    2\n",
      "\twow                 :    1\n",
      "\tweek                :    1\n",
      "\tumm                 :    1\n",
      "\ttri                 :    1\n",
      "\ttoday arriv         :    1\n",
      "\ttoday               :    1\n",
      "\tswab everyth        :    1\n",
      "\tstick               :    1\n",
      "\tstaff               :    1\n",
      "----------     US Airways     ----------\n",
      "\tflight              :  794\n",
      "\tthank               :  244\n",
      "\tservic              :  196\n",
      "\tcustom              :  184\n",
      "\tcancel              :  167\n",
      "\thour                :  156\n",
      "\thold                :  148\n",
      "\ttime                :  144\n",
      "\tdelay               :  129\n",
      "\tcustom servic       :  128\n"
     ]
    }
   ],
   "source": [
    "airline_names = [\n",
    "    \"American Air\", \"Delta\", \"Southwest Air\", \"United\", \"Virgin America\",\n",
    "    \"Air France\", \"British Airways\", \"US Airways\"\n",
    "]\n",
    "\n",
    "for data_airline, airline in zip(data_only_auth, airline_names):\n",
    "    print(\"-\" * 10 + \"{:^20s}\".format(airline) + \"-\" * 10)\n",
    "    for c, w in n_frequent(data_airline):\n",
    "        print(\"\\t{:20s}:{:5d}\".format(w, c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Question 13 (1 point)\n",
    "\n",
    "The table generated in the Question 12 can lead us to any conclusion about each one of the 9 companies? Can we identify specific events that have occured during the data retrieval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Air France, \"sad\", \"miss\", and \"crash\" must refer to the accident that occured in 2009.\n",
    "Every company has a lot of \"cancel\" or \"cancel flight\" which makes sense (you *usually* tweet when you are disappointed about a service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Bonus (2 points)\n",
    "\n",
    "Person names, companies names and locations are called named entities. Named-entity recognition (NER) is the task of extracting named entities  classifying them using pre-defined categories. In this bonus section, you will use a Named Entity Recognizer to automatically extract named entities from the tweets. This approach is generic enough to retrieve information about other companies or even product and people names.\n",
    "\n",
    "**For the bonus, you are free to use any Named Entity Recognizer that has python wrapper or is implemented in python. Moreover, you have to use the tweet database of the previous section (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Bonus 2 (1 point)\n",
    "\n",
    "Implement a code that generates the table with the top 10 most mentioned named entities in the database (this table has to contain the frequencies of the name entities). After that, generates a bar chart that despicts the number of positive, negative and neutral tweets for each one of these 10 named entities. Briefly describe the results found in the bar chart.\n",
    "\n",
    "*Ignore the named entities related to the following airline companies : Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/quentin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/quentin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ne(tweet):\n",
    "    airline_names = [\n",
    "    \"@americanair\", \"#americanair\", \"american air\", \"american_air\", \"@delta\",\n",
    "    \"#delta\", \"delta air\", \"delta_air\", \"@southwestair\", \"#southwestair\",\n",
    "    \"southwest air\", \"southwest_air\", \"@united\", \"#united\", \"united air\",\n",
    "    \"united_air\", \"@virginamerica\", \"#virginamerica\", \"virgin america\",\n",
    "    \"virgin_america\", \"@airfrance\", \"#airfrance\", \"air france\", \"air_france\",\n",
    "    \"@british_airways\", \"#british_airways\", \"british air\", \"@usairways\",\n",
    "    \"#usairways\", \"us air\", \"us_air\"\n",
    "    ]\n",
    "    \n",
    "    tagged_tweet = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(tweet)))\n",
    "    ne_tag = [\"ORGANIZATION\", \"PERSON\", \"LOCATION\", \"FACILITY\", \"GPE\", \"GSP\"]\n",
    "    ne = []\n",
    "    for chunk in tagged_tweet:\n",
    "        if hasattr(chunk, 'label') and chunk.label() in ne_tag:\n",
    "            ne.append(' '.join(c[0] for c in chunk if c[0] not in airline_names))\n",
    "    return ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_entity = list(map(get_ne, tweets_list))\n",
    "entities, counts = np.unique([v for ne in name_entity for v in ne if ne], return_counts=True)\n",
    "counts_entities = sorted(zip(counts, entities), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good                :       6937\n",
      "LOL                 :       5470\n",
      "Sorry               :       5221\n",
      "Twitter             :       4258\n",
      "Happy               :       4093\n",
      "Great               :       3142\n",
      "iPhone              :       3032\n",
      "Haha                :       2571\n",
      "Got                 :       2289\n",
      "Thanks              :       2125\n"
     ]
    }
   ],
   "source": [
    "for c, e in counts_entities[:10]:\n",
    "    print(\"{:20s}: {:10d}\".format(e, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_ne = list(list(zip(*counts_entities[:10]))[1])\n",
    "tweets_to_classify = [[] for _ in range(10)]\n",
    "\n",
    "for tweet, ne in zip(tweets_list, name_entity):\n",
    "    if ne:\n",
    "        for i, e in enumerate(top10_ne):\n",
    "            if e in ne:\n",
    "                tweets_to_classify[i].append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_to_classify = [bowObj.transform(tweets) for tweets in tweets_to_classify]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [extract_sentiment(classifier, bow) for bow in bow_to_classify]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    unique, counts = numpy.unique(r, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        if u == 0:\n",
    "            plt.bar(i - 0.25, c, 0.25, color=\"red\")\n",
    "        if u == 1:\n",
    "            plt.bar(i, c, 0.25, color=\"grey\")\n",
    "        if u == 2:\n",
    "            plt.bar(i + 0.25, c, 0.25, color=\"blue\")\n",
    "\n",
    "plt.xticks(list(range(len(top10_ne))))\n",
    "plt.gca().set_xticklabels(top10_ne)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that \"good\" , \"happy\" and \"great\" are very positive. Similarly, \"Sorry\" is very negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Bonus 3 (1 point)\n",
    "\n",
    "Generate a similar table produced in the Question 12 for the 10 most mentioned named entities in Bonus 2. Can we draw any conclusion about these named entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
