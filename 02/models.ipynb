{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need python 3.7 to have use datetime.datetime.fromisoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import numpy as np\n",
    "from functions import split\n",
    "from functions import pipeline\n",
    "from functions import load_data\n",
    "from functions import compute_f1\n",
    "from functions import corr_matrix\n",
    "from functions import plot_feature\n",
    "from functions import print_sample\n",
    "from functions import convert_date\n",
    "from functions import convert_type\n",
    "from functions import print_feature\n",
    "from functions import remove_missing\n",
    "from functions import delete_feature\n",
    "from functions import convert_one_hot\n",
    "from functions import convert_weather\n",
    "from functions import feature_output_corr\n",
    "from functions import normalization_feature\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* path : (STRING) path of the file to load.\n",
    "* limit : (INT) limit the number of example to load.\n",
    "* delete_features : (LIST) feature names to remove.\n",
    "* cvrt_date : (BOOLEAN) convert the data\n",
    "* weather : (LIST) weather to consider. All other will be dropped.\n",
    "* one_hot_features : (LIST) feature names to convert in one-hot vector.\n",
    "* norm_features : (LIST) feature names to normalize in one-hot vector\n",
    "* missing_features (LIST) feature which missing values are to replace \n",
    "* missing_values   (LIST) value with which to replace the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded (9.9s)\n",
      "Visility indicator deleted (8.2s)\n",
      "hmdx deleted (7.3s)\n",
      "Wind Chill deleted (9.6s)\n",
      "Date splited in Year/Month/Day/Hour/Weekday (11.9s)\n",
      "Weekday converted in one-hot vector (21.6s)\n",
      "Weather converted (36.8s)\n",
      "Replace missing values (0.3s)\n",
      "Remove samples with missing values (1.7s)\n",
      "Data converted to float (27.7s)\n",
      "Temperature (째C) normalized (28.6s)\n",
      "Drew point (째C) normalized (29.1s)\n",
      "Relativite humidity (%) normalized (28.1s)\n",
      "wind direction (10s deg) normalized (27.3s)\n",
      "Wind speed (km/h) normalized (26.3s)\n",
      "Pressure at the station (kPa) normalized (31.3s)\n",
      "Visibility (km) normalized (29.8s)\n",
      "Sort data according to station code (1.2s)\n",
      "split data into x, y, and label (79.6s)\n"
     ]
    }
   ],
   "source": [
    "header, x, y, label = pipeline(path=\"data/training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = header.index(\"Station Code\")\n",
    "stations = list(set(list(zip(*x))[index]))\n",
    "x_stations = [[] for _ in stations]\n",
    "y_stations = [[] for _ in stations]\n",
    "label_stations = [[] for _ in stations]\n",
    "\n",
    "for _x, _y, _label in zip(x, y, label):\n",
    "    s = stations.index(_x[index])\n",
    "    x_stations[s].append(_x)\n",
    "    y_stations[s].append(_y)\n",
    "    label_stations[s].append(_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded (1.6s)\n",
      "Visility indicator deleted (3.1s)\n",
      "hmdx deleted (2.6s)\n",
      "Wind Chill deleted (2.5s)\n",
      "Date splited in Year/Month/Day/Hour/Weekday (4.6s)\n",
      "Weekday converted in one-hot vector (4.3s)\n",
      "Weather converted (7.8s)\n",
      "Replace missing values (0.1s)\n",
      "Remove samples with missing values (0.4s)\n",
      "Data converted to float (6.6s)\n",
      "Temperature (째C) normalized (6.7s)\n",
      "Drew point (째C) normalized (8.6s)\n",
      "Relativite humidity (%) normalized (6.5s)\n",
      "wind direction (10s deg) normalized (6.6s)\n",
      "Wind speed (km/h) normalized (6.4s)\n",
      "Pressure at the station (kPa) normalized (6.8s)\n",
      "Visibility (km) normalized (6.7s)\n",
      "Sort data according to station code (0.3s)\n"
     ]
    }
   ],
   "source": [
    "header_test, x_test = pipeline(path=\"data/test.csv\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = header_test.index(\"Station Code\")\n",
    "x_test_stations = [[] for _ in stations]\n",
    "\n",
    "for _x in x_test:\n",
    "    s = stations.index(_x[index])\n",
    "    x_test_stations[s].append(_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model logistic par station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = header_test.index(\"Year\")\n",
    "mi = header_test.index(\"Month\")\n",
    "di = header_test.index(\"Day\")\n",
    "hi = header_test.index(\"Hour\")\n",
    "si = header_test.index(\"Station Code\")\n",
    "\n",
    "with open(\"data/results.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow([\"id\",\"volume\"])\n",
    "    \n",
    "    for i, s in enumerate(stations):\n",
    "        # if empty (no recording for that station in test set)\n",
    "        if not x_test_stations[i]:\n",
    "            continue\n",
    "        model = LogisticRegression(max_iter=9999, class_weight={0: 1, 1: 6}, solver=\"lbfgs\")\n",
    "        model.fit(x_stations[i], label_stations[i])\n",
    "        pred = model.predict(x_test_stations[i])\n",
    "        \n",
    "        for i, (e, p) in enumerate(zip(x_test_stations[i], pred)):\n",
    "            d = \"2016-{:02d}-{:02d}_{:02d}:00_{:4d}\".format(int(e[mi]),int(e[di]),int(e[hi]),int(e[si]))\n",
    "            writer.writerow([d, str(bool(p))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header=np.asarray(header)\n",
    "x=np.asarray(x)\n",
    "y=np.asarray(y)\n",
    "label=np.asarray(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantile at 0.99:\", np.quantile(y, 0.99))\n",
    "print(\"10 Highest values:\", sorted(y, reverse = True)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_out_ind = np.where(y < 50)\n",
    "x, y, label = x[non_out_ind], y[non_out_ind], label[non_out_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of the withdrawals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=75)\n",
    "sns.distplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the tail of the distribution, we apply the square root function to to $y$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.sqrt(y)\n",
    "\n",
    "split = int(x.shape[0] * 0.8)\n",
    "x_train, x_valid = x[:split], x[split:]\n",
    "y_train, y_valid = y[:split], y[split:]\n",
    "label_train, label_valid = label[:split], label[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = LinearRegression(fit_intercept = True)\n",
    "    model_k.fit(X,Y)\n",
    "    RSS = mean_squared_error(Y,model_k.predict(X)) * len(Y)\n",
    "    R_squared = model_k.score(X,Y)\n",
    "    return RSS, R_squared\n",
    "\n",
    "\n",
    "#Initialization variables\n",
    "Y = y_train\n",
    "X = x_train\n",
    "k = 25\n",
    "# k = len(header)\n",
    "\n",
    "remaining_features = [i for i in range(len(header))]\n",
    "features = []\n",
    "RSS_list, R_squared_list = [np.inf], [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "for i in range(1,k+1):\n",
    "    best_RSS = np.inf\n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            RSS = fit_linear_reg(X[:,list(combo) + features],Y)   #Store temp result \n",
    "\n",
    "            if RSS[0] < best_RSS:\n",
    "                best_RSS = RSS[0]\n",
    "                best_R_squared = RSS[1] \n",
    "                best_feature = combo[0]\n",
    "\n",
    "    #Updating variables for next loop\n",
    "    features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    \n",
    "    #Saving values for plotting\n",
    "    RSS_list.append(best_RSS)\n",
    "    R_squared_list.append(best_R_squared)\n",
    "    features_list[i] = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,len(features_list)):\n",
    "    features_list[i] = [f for f in features_list[i] if header[f] in header_test]\n",
    "\n",
    "print('Forward stepwise subset selection')\n",
    "for i in range(1,20):\n",
    "    print('Number of features :', len(features_list[i]))\n",
    "    print('Features :', features_list[i])\n",
    "    print('RSS :', round(RSS_list[i]))\n",
    "    print('R Squared :', R_squared_list[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that we choose to keep :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_keep = features_list[10]\n",
    "\n",
    "x_train = x_train[:, features_keep]\n",
    "x_valid = x_valid[:, features_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression on Withdrawals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = LinearRegression()\n",
    "model_linear = model_linear.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_linear.predict(x_valid)\n",
    "\n",
    "# The coefficients\n",
    "print('Score: ', model_linear.score(x_valid, y_valid))\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_valid, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression between Withdrawals and Volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_logit = LogisticRegression(max_iter=9999, class_weight={0: 1, 1: 6})\n",
    "model_logit = model_logit.fit(y_train.reshape(-1,1), label_train)\n",
    "\n",
    "print(model_logit.score(y_valid.reshape(-1,1), label_valid))\n",
    "print(model_logit.score(model_linear.predict(x_valid).reshape(-1,1), label_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the result file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_keep_test = [np.where(i == header_test)[0][0] \n",
    "                      for i in header[features_keep]]\n",
    "x_test_f = x_test[:,features_keep_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model_logit.predict(model_linear.predict(x_test_f).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = np.where(header_test == \"Year\")\n",
    "mi = np.where(header_test == \"Month\")\n",
    "di = np.where(header_test == \"Day\")\n",
    "hi = np.where(header_test == \"Hour\")\n",
    "si = np.where(header_test == \"Station Code\")\n",
    "\n",
    "results = [[\"id\",\"volume\"]]\n",
    "\n",
    "for index,element in enumerate(x_test):\n",
    "    string_element = \"2016-{:02d}-{:02d}_{:02d}:00_{:4d}\".format(int(element[mi]),\n",
    "                                                                 int(element[di]),\n",
    "                                                                 int(element[hi]),\n",
    "                                                                 int(element[si]))\n",
    "    results.append([string_element, str(bool(label_pred[index]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(model_logit.predict(model_linear.predict(x_valid).reshape(-1,1))))\n",
    "print(sum(model_logit.predict(model_linear.predict(x_test_f).reshape(-1,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "with open(\"data/results.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
