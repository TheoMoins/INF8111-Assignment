{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need python 3.7 to have use datetime.datetime.fromisoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from functions import split\n",
    "from functions import pipeline\n",
    "from functions import load_data\n",
    "from functions import compute_f1\n",
    "from functions import corr_matrix\n",
    "from functions import plot_feature\n",
    "from functions import print_sample\n",
    "from functions import convert_date\n",
    "from functions import convert_type\n",
    "from functions import print_feature\n",
    "from functions import remove_missing\n",
    "from functions import delete_feature\n",
    "from functions import sort_by_station\n",
    "from functions import convert_one_hot\n",
    "from functions import convert_weather\n",
    "from functions import sort_by_duration\n",
    "from functions import feature_output_corr\n",
    "from functions import normalization_feature\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* path : (STRING) path of the file to load.\n",
    "* limit : (INT) limit the number of example to load.\n",
    "* delete_features : (LIST) feature names to remove.\n",
    "* cvrt_date : (BOOLEAN) convert the data\n",
    "* weather : (LIST) weather to consider. All other will be dropped.\n",
    "* one_hot_features : (LIST) feature names to convert in one-hot vector.\n",
    "* norm_features : (LIST) feature names to normalize in one-hot vector\n",
    "* missing_features (LIST) feature which missing values are to replace \n",
    "* missing_values   (LIST) value with which to replace the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded (3.6s)\n",
      "Visility indicator deleted (3.8s)\n",
      "hmdx deleted (4.3s)\n",
      "Wind Chill deleted (5.0s)\n",
      "Date splited in Year/Month/Day/Hour/Weekday (8.2s)\n",
      "Weather converted (15.8s)\n",
      "Replace missing values (0.2s)\n",
      "Remove samples with missing values (0.4s)\n",
      "Data converted to float (8.4s)\n",
      "Sort data according to station code (0.4s)\n",
      "split data into x, y, and label (34.1s)\n"
     ]
    }
   ],
   "source": [
    "header, x, y, label = pipeline(path=\"data/training.csv\",\n",
    "                               norm_features=[],\n",
    "                               one_hot_features=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, stations, x_stations, y_stations, label_stations = sort_by_station(\n",
    "    header, x, y, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded (0.6s)\n",
      "Visility indicator deleted (1.8s)\n",
      "hmdx deleted (1.2s)\n",
      "Wind Chill deleted (2.0s)\n",
      "Date splited in Year/Month/Day/Hour/Weekday (1.6s)\n",
      "Weather converted (3.1s)\n",
      "Replace missing values (0.0s)\n",
      "Remove samples with missing values (0.1s)\n",
      "Data converted to float (2.0s)\n",
      "Sort data according to station code (0.1s)\n"
     ]
    }
   ],
   "source": [
    "header_test, x_test = pipeline(path=\"data/test.csv\",\n",
    "                               norm_features=[],\n",
    "                               one_hot_features=[],\n",
    "                               test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, stations_test, x_test_stations = sort_by_station(header_test, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (stations == stations_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model logistic par station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 (0.94, 0.51)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "yi = header_test.index(\"Year\")\n",
    "mi = header_test.index(\"Month\")\n",
    "di = header_test.index(\"Day\")\n",
    "hi = header_test.index(\"Hour\")\n",
    "si = header_test.index(\"Station Code\")\n",
    "\n",
    "COMPUTE_THRESHOLD = True\n",
    "f1_train, f1_val = [], []\n",
    "\n",
    "with open(\"data/results.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow([\"id\", \"volume\"])\n",
    "\n",
    "    for i, s in enumerate(stations):\n",
    "        print(\"\\r{}/{} ({:.2f}, {:.2f})\".format(\n",
    "            i + 1, len(stations),\n",
    "            np.mean(f1_train) if f1_train or not COMPUTE_THRESHOLD else 0,\n",
    "            np.mean(f1_val) if f1_val or not COMPUTE_THRESHOLD else 0),\n",
    "              end=\"\")\n",
    "        # if empty (no recording for that station in test set)\n",
    "        if not x_test_stations[i]:\n",
    "            continue\n",
    "\n",
    "        # MODELS ALREADY TESTED\n",
    "#         model = LogisticRegression(max_iter=9999, class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "#         model = SVC(kernel=\"linear\",  class_weight=\"balanced\")\n",
    "#         model = LogisticRegression(penalty='l1', max_iter=9999, class_weight=\"balanced\", solver=\"saga\", n_jobs=-1)\n",
    "#         model = AdaBoostClassifier(LogisticRegression(max_iter=9999, class_weight=\"balanced\", solver=\"lbfgs\", n_jobs=-1), n_estimators=100)\n",
    "        model = RandomForestClassifier(n_estimators=50, max_depth=5, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "        if COMPUTE_THRESHOLD:\n",
    "            # sort by time\n",
    "            _x, _y, _label = sort_by_duration(header, x_stations[i],\n",
    "                                              y_stations[i], label_stations[i])\n",
    "            # create validation set\n",
    "            split = int(0.8 * len(_x))\n",
    "            x_train, x_valid = _x[:split], _x[split:]\n",
    "            y_train, y_valid = _y[:split], _y[split:]\n",
    "            label_train, label_valid = _label[:split], _label[split:]\n",
    "            # train model\n",
    "            model = model.fit(_x, _label)\n",
    "            # predict the probabilities for train and validation set\n",
    "            proba_train = list(zip(*model.predict_proba(x_train)))[1]\n",
    "            proba_valid = list(zip(*model.predict_proba(x_valid)))[1]\n",
    "            # compute best threshold on the validation set\n",
    "            f1_score, threshold = compute_f1(proba_valid, label_valid)\n",
    "            # print running average of f1-score for both train and eval\n",
    "            f1_val.append(f1_score)\n",
    "            f1_train.append(\n",
    "                np.mean([\n",
    "                    int(int(p > threshold) == l)\n",
    "                    for p, l in zip(proba_train, label_train)\n",
    "                ]))\n",
    "            # re-train model\n",
    "            model = model.fit(x_stations[i], label_stations[i])\n",
    "            # predict labels\n",
    "            proba_test = list(zip(*model.predict_proba(x_test_stations[i])))[1]\n",
    "            label_test = [1 if p > threshold else 0 for p in proba_test]\n",
    "        else:\n",
    "            # train model\n",
    "            model = model.fit(x_stations[i], label_stations[i])\n",
    "            # predict labels\n",
    "            label_test = model.predict(x_test_stations[i])\n",
    "\n",
    "        # write prediction in file\n",
    "        for i, (e, p) in enumerate(zip(x_test_stations[i], label_test)):\n",
    "            d = \"2016-{:02d}-{:02d}_{:02d}:00_{:4d}\".format(\n",
    "                int(e[mi]), int(e[di]), int(e[hi]), int(e[si]))\n",
    "            writer.writerow([d, str(bool(p))])\n",
    "print(\"\")\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
