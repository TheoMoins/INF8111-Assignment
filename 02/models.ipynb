{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need python 3.7 to have use datetime.datetime.fromisoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from functions import split\n",
    "from functions import pipeline\n",
    "from functions import load_data\n",
    "from functions import compute_f1\n",
    "from functions import corr_matrix\n",
    "from functions import plot_feature\n",
    "from functions import print_sample\n",
    "from functions import convert_date\n",
    "from functions import convert_type\n",
    "from functions import print_feature\n",
    "from functions import remove_missing\n",
    "from functions import delete_feature\n",
    "from functions import sort_by_station\n",
    "from functions import convert_one_hot\n",
    "from functions import convert_weather\n",
    "from functions import sort_by_duration\n",
    "from functions import feature_output_corr\n",
    "from functions import normalization_feature\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* path : (STRING) path of the file to load.\n",
    "* limit : (INT) limit the number of example to load.\n",
    "* delete_features : (LIST) feature names to remove.\n",
    "* cvrt_date : (BOOLEAN) convert the data\n",
    "* weather : (LIST) weather to consider. All other will be dropped.\n",
    "* one_hot_features : (LIST) feature names to convert in one-hot vector.\n",
    "* norm_features : (LIST) feature names to normalize in one-hot vector\n",
    "* missing_features (LIST) feature which missing values are to replace \n",
    "* missing_values   (LIST) value with which to replace the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded (19.4s)\n",
      "Visility indicator deleted (30.3s)\n",
      "hmdx deleted (26.9s)\n",
      "Wind Chill deleted (26.7s)\n",
      "Date splited in Year/Month/Day/Hour/Weekday (24.4s)\n",
      "Weather converted (22.3s)\n",
      "Weather rescaled (96.9s)\n",
      "Replace missing values (0.2s)\n",
      "Remove samples with missing values (0.4s)\n",
      "Data converted to float (9.1s)\n",
      "Sort data according to station code (0.6s)\n",
      "split data into x, y, and label (32.0s)\n"
     ]
    }
   ],
   "source": [
    "header, x, y, label = pipeline(path=\"data/training.csv\",\n",
    "                               norm_features=[],\n",
    "                               one_hot_features=[],\n",
    "                               weather_coef = [0, 5, 4, 9, 8, 2, 3, 0, 10, 7, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, stations, x_stations, y_stations, label_stations = sort_by_station(\n",
    "    header, x, y, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded (0.7s)\n",
      "Visility indicator deleted (1.4s)\n",
      "hmdx deleted (1.4s)\n",
      "Wind Chill deleted (2.3s)\n",
      "Date splited in Year/Month/Day/Hour/Weekday (1.7s)\n",
      "Weather converted (3.6s)\n",
      "Weather rescaled (22.4s)\n",
      "Replace missing values (0.0s)\n",
      "Remove samples with missing values (0.1s)\n",
      "Data converted to float (1.4s)\n",
      "Sort data according to station code (0.1s)\n"
     ]
    }
   ],
   "source": [
    "header_test, x_test = pipeline(path=\"data/test.csv\",\n",
    "                               norm_features=[],\n",
    "                               one_hot_features=[],\n",
    "                               test=True,\n",
    "                               weather_coef = [0, 5, 4, 9, 8, 2, 3, 0, 10, 7, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, stations_test, x_test_stations = sort_by_station(header_test, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (stations == stations_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model logistic par station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_value(header, data):\n",
    "    ind_row = 0\n",
    "    ind_col = header.index(\"Weather_Coef\")\n",
    "    for ind, d in enumerate(data):\n",
    "        if d[ind_col] == -1:\n",
    "            continue\n",
    "        else:\n",
    "            for inter in range(ind_row+1, ind):\n",
    "                alpha = (inter-ind_row)/(ind-ind_row)\n",
    "                data[inter][ind_col] = (1-alpha)*data[ind_row][ind_col]+alpha*data[ind][ind_col]\n",
    "            ind_row = ind\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 400, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 30, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "unique_model = True\n",
    "\n",
    "if unique_model:\n",
    "    \n",
    "    _x, _y, _label = sort_by_duration(header, x, y, label)\n",
    "    split = int(0.8 * len(_x))\n",
    "\n",
    "    model = RandomizedSearchCV(estimator = RandomForestClassifier(class_weight=\"balanced\", n_jobs=1), \n",
    "                                   param_distributions = random_grid, \n",
    "                                   n_iter = 50, \n",
    "                                   cv = zip([range(split)],[range(split,len(_x))]), \n",
    "                                   verbose=2, \n",
    "                                   n_jobs = -1)\n",
    "\n",
    "    if \"Weather_Coef\" in header:\n",
    "        _x = fill_missing_value(header, _x)\n",
    "\n",
    "    # train model\n",
    "    model = model.fit(_x, _label)\n",
    "    print(model.best_params_)\n",
    "\n",
    "else:\n",
    "\n",
    "    best_params_stations = []\n",
    "\n",
    "    for i in range(len(stations)):\n",
    "\n",
    "        _x, _y, _label = sort_by_duration(header, x_stations[i],y_stations[i], label_stations[i])\n",
    "        split = int(0.8 * len(_x))\n",
    "\n",
    "        model = RandomizedSearchCV(estimator = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1), \n",
    "                                       param_distributions = random_grid, \n",
    "                                       n_iter = 50, \n",
    "                                       cv = zip([range(split)],[range(split,len(_x))]), \n",
    "                                       verbose=0, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "        if \"Weather_Coef\" in header:\n",
    "            _x = fill_missing_value(header, _x)\n",
    "\n",
    "        # train model\n",
    "        model = model.fit(_x, _label)\n",
    "        best_params_stations.append(model.best_params_)\n",
    "        print(\"\\r{}/{}\".format(i+1, len(stations)),end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 43.9min\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 80.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 146, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 22}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 400, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 30, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 (0.96, 0.50)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "yi = header_test.index(\"Year\")\n",
    "mi = header_test.index(\"Month\")\n",
    "di = header_test.index(\"Day\")\n",
    "hi = header_test.index(\"Hour\")\n",
    "si = header_test.index(\"Station Code\")\n",
    "\n",
    "COMPUTE_THRESHOLD = True\n",
    "f1_train, f1_val = [], []\n",
    "\n",
    "with open(\"data/results.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow([\"id\", \"volume\"])\n",
    "\n",
    "    for i, s in enumerate(stations):\n",
    "        print(\"\\r{}/{} ({:.2f}, {:.2f})\".format(\n",
    "            i + 1, len(stations),\n",
    "            np.mean(f1_train) if f1_train or not COMPUTE_THRESHOLD else 0,\n",
    "            np.mean(f1_val) if f1_val or not COMPUTE_THRESHOLD else 0),\n",
    "              end=\"\")\n",
    "        # if empty (no recording for that station in test set)\n",
    "        if not x_test_stations[i]:\n",
    "            continue\n",
    "\n",
    "        # MODELS ALREADY TESTED\n",
    "#         model = LogisticRegression(max_iter=9999, class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "#         model = SVC(kernel=\"linear\",  class_weight=\"balanced\")\n",
    "#         model = LogisticRegression(penalty='l1', max_iter=9999, class_weight=\"balanced\", solver=\"saga\", n_jobs=-1)\n",
    "#         model = AdaBoostClassifier(LogisticRegression(max_iter=9999, class_weight=\"balanced\", solver=\"lbfgs\", n_jobs=-1), n_estimators=100)\n",
    "#         model = RandomForestClassifier(n_estimators=200, max_depth=None, n_jobs=-1, class_weight=\"balanced\")\n",
    "#         model = RandomForestClassifier(n_estimators=best_params_stations[i]['n_estimators'],\n",
    "#                                        max_features=best_params_stations[i]['max_features'],\n",
    "#                                        max_depth=best_params_stations[i]['max_depth'],\n",
    "#                                        min_samples_split=best_params_stations[i]['min_samples_split'],\n",
    "#                                        min_samples_leaf=best_params_stations[i]['min_samples_leaf'],\n",
    "#                                        n_jobs=-1, class_weight=\"balanced\")\n",
    "        model = RandomForestClassifier(n_estimators=146,\n",
    "                                       max_features='log2',\n",
    "                                       max_depth=22,\n",
    "                                       min_samples_split=3,\n",
    "                                       min_samples_leaf=5,\n",
    "                                       n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "        if COMPUTE_THRESHOLD:\n",
    "            # sort by time\n",
    "            _x, _y, _label = sort_by_duration(header, x_stations[i],\n",
    "                                              y_stations[i], label_stations[i])\n",
    "            \n",
    "            if \"Weather_Coef\" in header:\n",
    "                _x = fill_missing_value(header, _x)\n",
    "            \n",
    "            # create validation set\n",
    "            split = int(0.8 * len(_x))\n",
    "            x_train, x_valid = _x[:split], _x[split:]\n",
    "            y_train, y_valid = _y[:split], _y[split:]\n",
    "            label_train, label_valid = _label[:split], _label[split:]\n",
    "            # train model\n",
    "            model = model.fit(x_train, label_train)\n",
    "    \n",
    "            # predict the probabilities for train and validation set\n",
    "            proba_train = list(zip(*model.predict_proba(x_train)))[1]\n",
    "            proba_valid = list(zip(*model.predict_proba(x_valid)))[1]\n",
    "            \n",
    "            # compute best threshold on the validation set\n",
    "            f1_score, threshold = compute_f1(proba_valid, label_valid)\n",
    "            # print running average of f1-score for both train and eval\n",
    "            f1_val.append(f1_score)\n",
    "            f1_train.append(\n",
    "                np.mean([\n",
    "                    int(int(p > threshold) == l)\n",
    "                    for p, l in zip(proba_train, label_train)\n",
    "                ]))\n",
    "            \n",
    "            # re-train model\n",
    "            model = model.fit(_x, _label)\n",
    "\n",
    "            # predict labels\n",
    "            _x_test, _, _ = sort_by_duration(header_test, x_test_stations[i])\n",
    "            \n",
    "            if \"Weather_Coef\" in header_test:\n",
    "                _x_test = fill_missing_value(header_test, _x_test)\n",
    "    \n",
    "            proba_test = list(zip(*model.predict_proba(_x_test)))[1]\n",
    "            label_test = [1 if p > threshold else 0 for p in proba_test]\n",
    "        else:\n",
    "            # train model\n",
    "            model = model.fit(_x, _label)\n",
    "            # predict labels\n",
    "            label_test = model.predict(_x_test)\n",
    "\n",
    "        # write prediction in file\n",
    "        for i, (e, p) in enumerate(zip(x_test_stations[i], label_test)):\n",
    "            d = \"2016-{:02d}-{:02d}_{:02d}:00_{:4d}\".format(\n",
    "                int(e[mi]), int(e[di]), int(e[hi]), int(e[si]))\n",
    "            writer.writerow([d, str(bool(p))])\n",
    "print(\"\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
